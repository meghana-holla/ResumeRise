{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Representation \n",
    "\n",
    "The classifiers and learning algorithms can not directly process the text documents in their original form,as most of them expect numerical feature vectors with a fixed size rather than raw text docs with variable length. Therefore , during the preprocessing step, the texts are converted to a more manageable representation.\n",
    "\n",
    "One common approach for extracting features from text is to use the bag of words model: a model where for each document, a resume in our case, the presence (and often the frequency) of words is taken into consideration, but the order in which they occur is ignored. \n",
    "\n",
    "TermFrequency and InverseDocumentFrequency is used for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>HR</td>\n",
       "      <td>john h. smith p.h.r 800-991-5187 po box 1673 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>HR</td>\n",
       "      <td>name surname address mobile no/email personal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>HR</td>\n",
       "      <td>anthony brown hr assistant areas of expertise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>HR</td>\n",
       "      <td>www.downloadmela.com satheesh email id career ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>HR</td>\n",
       "      <td>human resources director expert in organizatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ID Category                                             Resume\n",
       "0           0   1       HR  john h. smith p.h.r 800-991-5187 po box 1673 c...\n",
       "1           1   2       HR  name surname address mobile no/email personal ...\n",
       "2           2   3       HR  anthony brown hr assistant areas of expertise ...\n",
       "3           3   4       HR  www.downloadmela.com satheesh email id career ...\n",
       "4           4   5       HR  human resources director expert in organizatio..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('clean_data1.csv')\n",
    "df = df.drop(['Resume'],axis=1)\n",
    "df.rename(columns={'newer_res':'Resume'},inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data and adding in ID for category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "col = ['Category', 'Resume']\n",
    "df = df[col]\n",
    "df = df[pd.notnull(df['Resume'])]\n",
    "df.columns = ['Category', 'Resume']\n",
    "df['category_id'] = df['Category'].factorize()[0]\n",
    "category_id_df = df[['Category', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'Category']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1199, 23338)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1,2), stop_words='english')\n",
    "features = tfidf.fit_transform(df.Resume).toarray()\n",
    "labels = df.category_id\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using chi2 to see correlated items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'Accountant':\n",
      "  . Most correlated unigrams:\n",
      "\t. chartered\n",
      "\t. accountant\n",
      "  . Most correlated bigrams:\n",
      "\t. chartered accountant\n",
      "\t. accountant resume\n",
      "\n",
      "\n",
      "\n",
      "# 'Advocate':\n",
      "  . Most correlated unigrams:\n",
      "\t. legal\n",
      "\t. law\n",
      "  . Most correlated bigrams:\n",
      "\t. school law\n",
      "\t. law school\n",
      "\n",
      "\n",
      "\n",
      "# 'Agricultural':\n",
      "  . Most correlated unigrams:\n",
      "\t. agriculture\n",
      "\t. horticulture\n",
      "  . Most correlated bigrams:\n",
      "\t. pest control\n",
      "\t. achievement american\n",
      "\n",
      "\n",
      "\n",
      "# 'Apparel':\n",
      "  . Most correlated unigrams:\n",
      "\t. mica\n",
      "\t. fashion\n",
      "  . Most correlated bigrams:\n",
      "\t. interior design\n",
      "\t. space planning\n",
      "\n",
      "\n",
      "\n",
      "# 'Architects':\n",
      "  . Most correlated unigrams:\n",
      "\t. tower\n",
      "\t. drawings\n",
      "  . Most correlated bigrams:\n",
      "\t. prepare drawings\n",
      "\t. auto cad\n",
      "\n",
      "\n",
      "\n",
      "# 'Arts':\n",
      "  . Most correlated unigrams:\n",
      "\t. artist\n",
      "\t. theatre\n",
      "  . Most correlated bigrams:\n",
      "\t. artist resume\n",
      "\t. make artist\n",
      "\n",
      "\n",
      "\n",
      "# 'Automobile':\n",
      "  . Most correlated unigrams:\n",
      "\t. automobile\n",
      "\t. automotive\n",
      "  . Most correlated bigrams:\n",
      "\t. automobile engineering\n",
      "\t. experience automotive\n",
      "\n",
      "\n",
      "\n",
      "# 'Aviation':\n",
      "  . Most correlated unigrams:\n",
      "\t. attendant\n",
      "\t. flight\n",
      "  . Most correlated bigrams:\n",
      "\t. attendant resume\n",
      "\t. flight attendant\n",
      "\n",
      "\n",
      "\n",
      "# 'BPO':\n",
      "  . Most correlated unigrams:\n",
      "\t. ymail\n",
      "\t. bpo\n",
      "  . Most correlated bigrams:\n",
      "\t. ymail com\n",
      "\t. experience center\n",
      "\n",
      "\n",
      "\n",
      "# 'Banking':\n",
      "  . Most correlated unigrams:\n",
      "\t. bank\n",
      "\t. banking\n",
      "  . Most correlated bigrams:\n",
      "\t. experiences 2008\n",
      "\t. career experiences\n",
      "\n",
      "\n",
      "\n",
      "# 'Building & Construction':\n",
      "  . Most correlated unigrams:\n",
      "\t. demolition\n",
      "\t. construction\n",
      "  . Most correlated bigrams:\n",
      "\t. site safety\n",
      "\t. construction supervisor\n",
      "\n",
      "\n",
      "\n",
      "# 'Business Development':\n",
      "  . Most correlated unigrams:\n",
      "\t. jad\n",
      "\t. temple\n",
      "  . Most correlated bigrams:\n",
      "\t. administration graduation\n",
      "\t. business analyst\n",
      "\n",
      "\n",
      "\n",
      "# 'Consultant':\n",
      "  . Most correlated unigrams:\n",
      "\t. consultant\n",
      "\t. seoul\n",
      "  . Most correlated bigrams:\n",
      "\t. management consulting\n",
      "\t. management consultant\n",
      "\n",
      "\n",
      "\n",
      "# 'Designing':\n",
      "  . Most correlated unigrams:\n",
      "\t. graphic\n",
      "\t. designer\n",
      "  . Most correlated bigrams:\n",
      "\t. graphic design\n",
      "\t. graphic designer\n",
      "\n",
      "\n",
      "\n",
      "# 'Digital Media':\n",
      "  . Most correlated unigrams:\n",
      "\t. content\n",
      "\t. media\n",
      "  . Most correlated bigrams:\n",
      "\t. digital marketing\n",
      "\t. social media\n",
      "\n",
      "\n",
      "\n",
      "# 'Education':\n",
      "  . Most correlated unigrams:\n",
      "\t. elementary\n",
      "\t. teacher\n",
      "  . Most correlated bigrams:\n",
      "\t. elementary school\n",
      "\t. special education\n",
      "\n",
      "\n",
      "\n",
      "# 'Engineering':\n",
      "  . Most correlated unigrams:\n",
      "\t. matlab\n",
      "\t. engineering\n",
      "  . Most correlated bigrams:\n",
      "\t. chemical engineering\n",
      "\t. mechanical engineering\n",
      "\n",
      "\n",
      "\n",
      "# 'Finance':\n",
      "  . Most correlated unigrams:\n",
      "\t. financial\n",
      "\t. finance\n",
      "  . Most correlated bigrams:\n",
      "\t. finance manager\n",
      "\t. financial analyst\n",
      "\n",
      "\n",
      "\n",
      "# 'Food & Beverages':\n",
      "  . Most correlated unigrams:\n",
      "\t. culinary\n",
      "\t. chef\n",
      "  . Most correlated bigrams:\n",
      "\t. culinary arts\n",
      "\t. food service\n",
      "\n",
      "\n",
      "\n",
      "# 'HR':\n",
      "  . Most correlated unigrams:\n",
      "\t. downloadmela\n",
      "\t. hr\n",
      "  . Most correlated bigrams:\n",
      "\t. downloadmela com\n",
      "\t. employee relations\n",
      "\n",
      "\n",
      "\n",
      "# 'Health & Fitness':\n",
      "  . Most correlated unigrams:\n",
      "\t. patient\n",
      "\t. patients\n",
      "  . Most correlated bigrams:\n",
      "\t. patient care\n",
      "\t. health care\n",
      "\n",
      "\n",
      "\n",
      "# 'Information Technology':\n",
      "  . Most correlated unigrams:\n",
      "\t. java\n",
      "\t. developer\n",
      "  . Most correlated bigrams:\n",
      "\t. computer science\n",
      "\t. software engineer\n",
      "\n",
      "\n",
      "\n",
      "# 'Managment':\n",
      "  . Most correlated unigrams:\n",
      "\t. manager\n",
      "\t. pmp\n",
      "  . Most correlated bigrams:\n",
      "\t. manager resume\n",
      "\t. manager 2003\n",
      "\n",
      "\n",
      "\n",
      "# 'Public Relations':\n",
      "  . Most correlated unigrams:\n",
      "\t. appalachian\n",
      "\t. insert\n",
      "  . Most correlated bigrams:\n",
      "\t. sales supervisor\n",
      "\t. service resume\n",
      "\n",
      "\n",
      "\n",
      "# 'Sales':\n",
      "  . Most correlated unigrams:\n",
      "\t. gadgets\n",
      "\t. sales\n",
      "  . Most correlated bigrams:\n",
      "\t. analyzing monthly\n",
      "\t. generating analyzing\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 2\n",
    "for Category, category_id in sorted(category_to_id.items()):\n",
    "    features_chi2 = chi2(features, labels == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    #print(feature_names)\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    #trigrams = [v for v in feature_names if len(v.split(' ')) == 3] \n",
    "    print(\"# '{}':\".format(Category))\n",
    "    print(\"  . Most correlated unigrams:\\n\\t. {}\".format('\\n\\t. '.join(unigrams[-N:])))\n",
    "    print(\"  . Most correlated bigrams:\\n\\t. {}\".format('\\n\\t. '.join(bigrams[-N:])))\n",
    "    print(\"\\n\\n\")\n",
    "    #print(\"  . Most correlated trigrams:\\n. {}\".format('\\n. '.join(trigrams[-N:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Why :** \n",
    "\n",
    "Suppose there are N instances, and two classes(say A and B).Given a feature X, we can use Chi Square Test to evaluate its importance to distinguish between the classes. \n",
    "By calculating the Chi square scores for all the features, we can rank the features by the chi square scores, then choose the top ranked features for model training. \n",
    "**Chi Square Test is used in statistics to test the independence of two events.\n",
    "In feature selection part of this project , the two events are :** \n",
    "\n",
    "**1.Occurence of a feature**\n",
    "\n",
    "**2.Occurence of a Class/Doc category** \n",
    "\n",
    "**Note:** \n",
    "the higher value of the chi^2 score, the more likelihood the feature is correlated with the class, thus it should be selected for model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Classifier: Features and Design\n",
    "\n",
    "To train supervised classifiers, we first transformed the “Resumes” into a vector of numbers. We explored vector representations such as TF-IDF weighted vectors and also made sure there is some kind of correlation using the Chi^2 test to confirm that predictions are possible with these features that can be extracted from the documents. \n",
    "\n",
    "After having this vector representations of the text we can train supervised classifiers to train unseen “Resumes” and predict the “Job Category” on which they fall. After all the above data transformation, now that we have all the features and labels, it is time to train the classifiers. There are a number of algorithms we can use for this type of problem. \n",
    "\n",
    "\n",
    "Naive Bayes Classifier: the one most suitable for word counts is the multinomial variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Resume'], df['Category'], random_state = 0)\n",
    "\n",
    "#print(x_train)\n",
    "\n",
    "count_vect = CountVectorizer() # bag-of-ngrams model , based on frequency count\n",
    "x_train_counts = count_vect.fit_transform(x_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer() #passing the word:word count\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)\n",
    "\n",
    "classifier = MultinomialNB().fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing it on an unseen pdf resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "def convertPDFtoText(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    string = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dushyant Bhatt\n",
      "\n",
      "BI / Big Data/ Azure\n",
      "\n",
      "Hyderabad-Deccan, Telangana, Telangana - Email me on Indeed: indeed.com/r/Dushyant-\n",
      "Bhatt/140749dace5dc26f\n",
      "\n",
      "• 10+ years of Experience in Designing, Development, Administration, Analysis, Management in\n",
      "the Business Intelligence Data warehousing, Client Server Technologies, Web-based Applications,\n",
      "cloud solutions and Databases.\n",
      "• Data warehouse: Data analysis, star/ snow flake schema data modeling and design specific to\n",
      "data warehousing and business intelligence environment.\n",
      "• Database: Experience in database designing, scalability, back-up and recovery, writing and\n",
      "optimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes. \n",
      "• Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure, Stream\n",
      "Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake analytics(U-SQL).\n",
      "• Big Data: Worked Azure data lake store/analytics for big data processing and Azure data factory\n",
      "to schedule U-SQL jobs. Designed and developed end to end big data solution for data insights. \n",
      "• BI:\n",
      "o  ETL:  Designed  and  developed  ETL  solution  in  SSIS.  Experience  in  Logging,  Error  handling,\n",
      "configuration, deployment, troubleshooting and performance tuning of SSIS Packages.\n",
      "o Reporting: Experience in all the Latest Reporting Tools like Tableau Data visualization, Power\n",
      "BI  and  SSRS  2012.  Act  as  a  Point  of  Contact  in  Data  Interoperability,  Analytics  and  BI  and\n",
      "Production Support issue resolution. Experience in Developing Performance Dashboards, Score\n",
      "cards, Metrics, what if analysis, Prompts, Drills. Reports/Dashboards for all the functional areas\n",
      "including Finance, Pricing, Purchasing and Sales/Marketing.\n",
      "\n",
      "Willing to relocate: Anywhere\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Software Engineer\n",
      "\n",
      "Microsoft  -  hyderbad, Telangana -\n",
      "\n",
      "December 2015 to Present\n",
      "\n",
      "1. Microsoft Rewards Live dashboards:\n",
      "Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping\n",
      "online. Microsoft Rewards members can earn points when searching with Bing, browsing with\n",
      "Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft\n",
      "Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft\n",
      "rewards  website.  Rewards  live  dashboards  gives  a  live  picture  of  usage  world-wide  and  by\n",
      "markets like US, Canada, Australia, new user registration count, top/bottom performing rewards\n",
      "offers, orders stats and weekly trends of user activities, orders and new user registrations. the\n",
      "PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes.\n",
      "Technology/Tools used\n",
      "• Event hub, stream analytics and Power BI.\n",
      "Responsibilities\n",
      "• Created stream analytics jobs to process event hub data\n",
      "\n",
      "\f",
      "• Created Power BI live dashboard to show live usage traffic, weekly trends, cards, charts to show\n",
      "top/bottom 10 offers and usage metrics.\n",
      "\n",
      "2. Microsoft Rewards Data Insights:\n",
      "Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping\n",
      "online. Microsoft Rewards members can earn points when searching with Bing, browsing with\n",
      "Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft\n",
      "Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft\n",
      "rewards website. Rewards data insights is data analytics and reporting platform, processes 20\n",
      "million users daily activities and redemption across different markets like US, Canada, Australia.\n",
      "Technology/Tools used\n",
      "• Cosmos (Microsoft big-data platform), c#, X-flow job monitoring, Power BI.\n",
      "Responsibilities\n",
      "• Created big data scripts in cosmos\n",
      "• C# data extractors, processors and reducers for data transformation\n",
      "• Power BI dashboards\n",
      "\n",
      "3. End to end tracking Tool:\n",
      "Description: - This is real-time Tracking tool to track different business transactions like order,\n",
      "order response, functional acknowledgement, invoice flowing inside ICOE. It gives flexibility to\n",
      "customers to track their transactions and appropriate error information in-case of any failure.\n",
      "Based on resource based access control the tool gives flexibility to end user to perform different\n",
      "actions like view transactions, search based on different filter criteria and view and download\n",
      "actual message payload. End to end tracking tool stitches all the business transaction like order\n",
      "to cash flow and connects different hops inside ICOE like gateway, routing server, Processing\n",
      "server. It also connects different systems like ICOE, partner end point and SAP.\n",
      "Technology/Tools used\n",
      "• Azure Document db, Azure web job and Web APP, RBAC, Angular JS.\n",
      "Responsibilities\n",
      "• Document dB stored procedures.\n",
      "• Web job to process event hub data and populate Document db\n",
      "• Web App API.\n",
      "• Stream analytics job to transform data\n",
      "• Power BI reports\n",
      "\n",
      "4. Biztrack Tracking Tool:\n",
      "Description: - This is real-time Tracking tool to track different business transactions like order,\n",
      "order response, functional acknowledgement, invoice flowing inside ICOE. It gives flexibility to\n",
      "customers to track their transactions and appropriate error information in-case of any failure.\n",
      "Based on resource based access control the tool gives flexibility to end user to perform different\n",
      "actions like view transactions, search based on different filter criteria and view and download\n",
      "actual message payload.\n",
      "Technology/Tools used\n",
      "• SQL server 2014, SSIS, .net API, Angular JS.\n",
      "Responsibilities\n",
      "• ETL solution to transform business transactions data stored in Biztalk tables.\n",
      "• SQL azure tables, stored procedures, User defined functions.\n",
      "• Performance tuning.\n",
      "• Web API enhancements.\n",
      "\n",
      "\f",
      "EDUCATION\n",
      "\n",
      "Saurashtra University  -  Morbi, Gujarat\n",
      "\n",
      "2007\n",
      "\n",
      "SKILLS\n",
      "\n",
      "problem solving (Less than 1 year), project lifecycle (Less than 1 year), project manager (Less\n",
      "than 1 year), technical assistance. (Less than 1 year)\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "Professional Skills\n",
      "• Excellent analytical, problem solving, communication, knowledge transfer and interpersonal\n",
      "skills with ability to interact with individuals at all the levels\n",
      "• Quick learner and maintains cordial relationship with project manager and team members and\n",
      "good performer both in team and independent job environments\n",
      "• Positive attitude towards superiors &amp; peers\n",
      "• Supervised junior developers throughout project lifecycle and provided technical assistance.\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "test_resume = convertPDFtoText(\"/home/manish/ACADEMICS/PROJECTS/ResumeRise/sample_input.pdf\")\n",
    "print(test_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Information Technology']\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict(count_vect.transform([test_resume])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
